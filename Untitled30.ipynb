{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A_xGVNp8o59a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB,ComplementNB,MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier,LocalOutlierFactor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(r'C:\\Users\\Bhavya\\Desktop\\HateSpeechDataset\\labeled_data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JKyxQPgSo7Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_and_extract_rt_username(text):\n",
        "    rt_pattern = re.search(r'RT\\s+@[\\w]+:', text)\n",
        "    if rt_pattern:\n",
        "        extracted_pattern = rt_pattern.group()\n",
        "        text_without_rt = text.replace(extracted_pattern, '')\n",
        "        return text_without_rt, extracted_pattern\n",
        "    else:\n",
        "        return text, None\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    hashtags = re.findall(r'#\\w+', text)\n",
        "    return hashtags\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "def extract_urls(text):\n",
        "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
        "    return urls\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    s = re.sub(r'[^\\w\\s]', '', s)\n",
        "    return s\n",
        "\n",
        "def remove_stopwords(s):\n",
        "    s = ' '.join(word for word in s.split() if word not in stop_words)\n",
        "    return s\n",
        "\n",
        "def remove_symbols(s):\n",
        "    s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
        "    return s\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def stem_text(s):\n",
        "    s = ' '.join(stemmer.stem(word) for word in s.split())\n",
        "    return s\n",
        "def lemmatize_text(s):\n",
        "    s = ' '.join(lemmatizer.lemmatize(word) for word in s.split())\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "UCmtcI4eo8xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Hashtags'] = df['tweet'].apply(extract_hashtags)\n",
        "df['text_without_hashtag'] = df['tweet'].apply(lambda x: re.sub(r'#\\w+', '', x))\n",
        "df['text_without_hashtag_rt'], df['rt_username'] = zip(*df['text_without_hashtag'].apply(remove_and_extract_rt_username))\n",
        "df['urls'] = df['text_without_hashtag_rt'].apply(extract_urls)\n",
        "df['text_without_urls'] = df['text_without_hashtag_rt'].apply(remove_urls)\n",
        "df['cleaned_text'] = df['text_without_hashtag_rt'].apply(remove_urls)\n",
        "df['cleaned_text2']=df['cleaned_text'].str.replace('[^\\w\\s]','')\n",
        "df['cleaned_text3']=df['cleaned_text2'].apply(str.lower)\n",
        "df['cleaned_text4']=df['cleaned_text3'].apply(remove_symbols)\n",
        "df['stemmed']=df['cleaned_text4'].apply(stem_text)\n",
        "df['lemmatized']=df['cleaned_text4'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "Jh9y5zKVo-Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "P8oPS2sfpGWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=df['cleaned_text4'].values\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(x)\n",
        "y=df['class']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=51,stratify=y)\n",
        "models = dict()\n",
        "results=dict()\n",
        "models['Logistic Regression'] = LogisticRegression(max_iter=5000)\n",
        "models['Multinomial Naive Bayes'] = MultinomialNB()\n",
        "models['Complement Naive Bayes'] = ComplementNB(force_alpha=True)\n",
        "models['KMeans'] = KMeans(n_clusters=2, n_init=10, random_state=59)\n",
        "models['XGB']= XGBClassifier(n_estimators=500)\n",
        "models['Support Vector Machine'] = SVC(kernel = 'sigmoid', gamma='scale',probability=True)\n",
        "models['Decision Tree'] = DecisionTreeClassifier(max_depth=100)\n",
        "models['kNN'] = KNeighborsClassifier()\n",
        "models['SGD']=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None)\n",
        "models['adaboost']=AdaBoostClassifier()\n",
        "models['Random Forest'] = RandomForestClassifier(n_estimators=100)\n",
        "print(\"Training Models:\\n\")\n",
        "for model in models:\n",
        "        models[model].fit(x_train,y_train)\n",
        "        print(\"model \"+str(model)+\" trained\")\n",
        "print(\"Test Set Prediction:\\n\")\n",
        "model_names,accuracies,precisions_w,recalls_w,f1_scores_w,precisions_m,recalls_m,f1_scores_m = [],[],[],[],[],[],[],[]\n",
        "for i in models:\n",
        "        print(i)\n",
        "        y_pred=models[i].predict(x_test)\n",
        "        print(classification_report(y_test,y_pred))\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        if i!='SGD':\n",
        "                results[models[i]]=accuracy\n",
        "        precision_w = precision_score(y_test, y_pred,average='weighted')\n",
        "        recall_w = recall_score(y_test, y_pred,average='weighted')\n",
        "        f1_w = f1_score(y_test, y_pred,average='weighted')\n",
        "        precision_m = precision_score(y_test, y_pred,average='macro')\n",
        "        recall_m = recall_score(y_test, y_pred,average='macro')\n",
        "        f1_m = f1_score(y_test, y_pred,average='macro')\n",
        "        model_names.append(i)\n",
        "        accuracies.append(accuracy)\n",
        "        precisions_w.append(precision_w)\n",
        "        recalls_w.append(recall_w)\n",
        "        f1_scores_w.append(f1_w)\n",
        "        precisions_m.append(precision_m)\n",
        "        recalls_m.append(recall_m)\n",
        "        f1_scores_m.append(f1_m)\n",
        "top_3_keys_bow = sorted(results, key=lambda k: results[k], reverse=True)[:3]\n",
        "c1=top_3_keys_bow[0]\n",
        "c2=top_3_keys_bow[1]\n",
        "c3=top_3_keys_bow[2]\n",
        "models['ensemble voting'] = VotingClassifier (estimators=[('clf1',c1), ('clf2',c2),('clf3',c3)], voting='soft')\n",
        "models['ensemble voting'].fit(x_train, y_train)\n",
        "y_pred = models['ensemble voting'].predict(x_test)\n",
        "print('-'*20+'VotingClassifier'+'-'*20)\n",
        "print(classification_report(y_test, y_pred,digits=5))\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_w = precision_score(y_test, y_pred,average='weighted')\n",
        "recall_w = recall_score(y_test, y_pred,average='weighted')\n",
        "f1_w = f1_score(y_test, y_pred,average='weighted')\n",
        "precision_m = precision_score(y_test, y_pred,average='macro')\n",
        "recall_m = recall_score(y_test, y_pred,average='macro')\n",
        "f1_m = f1_score(y_test, y_pred,average='macro')\n",
        "model_names.append('ensemble voting')\n",
        "accuracies.append(accuracy)\n",
        "precisions_w.append(precision_w)\n",
        "recalls_w.append(recall_w)\n",
        "f1_scores_w.append(f1_w)\n",
        "precisions_m.append(precision_m)\n",
        "recalls_m.append(recall_m)\n",
        "f1_scores_m.append(f1_m)\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model Name': model_names,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision Weighted': precisions_w,\n",
        "    'Recall Weighted': recalls_w,\n",
        "    'F1-Score Weighted': f1_scores_w,\n",
        "    'Precision Macro': precisions_m,\n",
        "    'Recall Macro': recalls_m,\n",
        "    'F1-Score Macro': f1_scores_m\n",
        "})\n",
        "metrics_df.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
        "metrics_df.to_csv(\"Bow.csv\")"
      ],
      "metadata": {
        "id": "pdsNwwAOpKdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=df['cleaned_text4'].values\n",
        "vectorizer = TfidfVectorizer()\n",
        "x = vectorizer.fit_transform(x)\n",
        "y=df['class']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=51,stratify=y)\n",
        "models = dict()\n",
        "results=dict()\n",
        "models['Linear Regression'] = LogisticRegression(max_iter=5000)\n",
        "models['Multinomial Naive Bayes'] = MultinomialNB()\n",
        "models['Complement Naive Bayes'] = ComplementNB(force_alpha=True)\n",
        "models['KMeans'] = KMeans(n_clusters=2, n_init=10, random_state=59)\n",
        "models['XGB']= XGBClassifier(n_estimators=500)\n",
        "models['Support Vector Machine'] = SVC(kernel = 'sigmoid', gamma='scale',probability=True)\n",
        "models['Decision Tree'] = DecisionTreeClassifier(max_depth=100)\n",
        "models['kNN'] = KNeighborsClassifier()\n",
        "models['SGD']=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None)\n",
        "models['adaboost']=AdaBoostClassifier()\n",
        "models['Random Forest'] = RandomForestClassifier(n_estimators=100)\n",
        "print(\"Training Models:\\n\")\n",
        "for model in models:\n",
        "        models[model].fit(x_train,y_train)\n",
        "        print(\"model \"+str(model)+\" trained\")\n",
        "print(\"Test Set Prediction:\\n\")\n",
        "model_names,accuracies,precisions_w,recalls_w,f1_scores_w,precisions_m,recalls_m,f1_scores_m = [],[],[],[],[],[],[],[]\n",
        "for i in models:\n",
        "        print(i)\n",
        "        y_pred=models[i].predict(x_test)\n",
        "        print(classification_report(y_test,y_pred))\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        if i!='SGD':\n",
        "                results[models[i]]=accuracy\n",
        "        precision_w = precision_score(y_test, y_pred,average='weighted')\n",
        "        recall_w = recall_score(y_test, y_pred,average='weighted')\n",
        "        f1_w = f1_score(y_test, y_pred,average='weighted')\n",
        "        precision_m = precision_score(y_test, y_pred,average='macro')\n",
        "        recall_m = recall_score(y_test, y_pred,average='macro')\n",
        "        f1_m = f1_score(y_test, y_pred,average='macro')\n",
        "        model_names.append(i)\n",
        "        accuracies.append(accuracy)\n",
        "        precisions_w.append(precision_w)\n",
        "        recalls_w.append(recall_w)\n",
        "        f1_scores_w.append(f1_w)\n",
        "        precisions_m.append(precision_m)\n",
        "        recalls_m.append(recall_m)\n",
        "        f1_scores_m.append(f1_m)\n",
        "top_3_keys_bow = sorted(results, key=lambda k: results[k], reverse=True)[:3]\n",
        "c1=top_3_keys_bow[0]\n",
        "c2=top_3_keys_bow[1]\n",
        "c3=top_3_keys_bow[2]\n",
        "models['ensemble voting'] = VotingClassifier (estimators=[('clf1',c1), ('clf2',c2),('clf3',c3)], voting='soft')\n",
        "models['ensemble voting'].fit(x_train, y_train)\n",
        "y_pred = models['ensemble voting'].predict(x_test)\n",
        "print('-'*20+'VotingClassifier'+'-'*20)\n",
        "print(classification_report(y_test, y_pred,digits=5))\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_w = precision_score(y_test, y_pred,average='weighted')\n",
        "recall_w = recall_score(y_test, y_pred,average='weighted')\n",
        "f1_w = f1_score(y_test, y_pred,average='weighted')\n",
        "precision_m = precision_score(y_test, y_pred,average='macro')\n",
        "recall_m = recall_score(y_test, y_pred,average='macro')\n",
        "f1_m = f1_score(y_test, y_pred,average='macro')\n",
        "model_names.append('ensemble voting')\n",
        "accuracies.append(accuracy)\n",
        "precisions_w.append(precision_w)\n",
        "recalls_w.append(recall_w)\n",
        "f1_scores_w.append(f1_w)\n",
        "precisions_m.append(precision_m)\n",
        "recalls_m.append(recall_m)\n",
        "f1_scores_m.append(f1_m)\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model Name': model_names,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision Weighted': precisions_w,\n",
        "    'Recall Weighted': recalls_w,\n",
        "    'F1-Score Weighted': f1_scores_w,\n",
        "    'Precision Macro': precisions_m,\n",
        "    'Recall Macro': recalls_m,\n",
        "    'F1-Score Macro': f1_scores_m\n",
        "})\n",
        "metrics_df.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
        "metrics_df.to_csv(\"TFIDF.csv\")"
      ],
      "metadata": {
        "id": "z8UTTyIfpM07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(r\"C:\\Users\\Bhavya\\Desktop\\HateSpeechDataset\\glove2.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "NRFr2qyPpPS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts=df['cleaned_text4'].astype(str)\n",
        "embeddings = []\n",
        "n=0\n",
        "ex=[]\n",
        "for text in texts:\n",
        "    text_embedding = []\n",
        "    for word in text:\n",
        "        if word in embeddings_index:\n",
        "            text_embedding.append(embeddings_index[word])\n",
        "    if len(text_embedding) > 0:\n",
        "        text_embedding = np.mean(text_embedding, axis=0)\n",
        "        embeddings.append(text_embedding)\n",
        "    else:\n",
        "        embeddings.append(np.zeros(300))\n",
        "    n+=1\n",
        "embeddings = np.array(embeddings)\n",
        "len(embeddings)\n",
        "\n",
        "y=df['class']\n",
        "x_train,x_test,y_train,y_test=train_test_split(embeddings,y,test_size=0.3,stratify=y,random_state=51)\n",
        "models = dict()\n",
        "results=dict()\n",
        "model_names,accuracies,precisions_w,recalls_w,f1_scores_w,precisions_m,recalls_m,f1_scores_m = [],[],[],[],[],[],[],[]\n",
        "models['Linear Regression'] = LogisticRegression(max_iter=5000)\n",
        "models['KMeans'] = KMeans(n_clusters=2, n_init=10, random_state=59)\n",
        "models['XGB']= XGBClassifier(n_estimators=5000)\n",
        "models['Support Vector Machine'] = SVC(kernel = 'sigmoid', gamma='scale',probability=True)\n",
        "models['Decision Tree'] = DecisionTreeClassifier(max_depth=100)\n",
        "models['kNN'] = KNeighborsClassifier()\n",
        "models['SGD']=SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None)\n",
        "for i in models:\n",
        "    models[i].fit(x_train,y_train)\n",
        "    print('-'*20+i+'-'*20)\n",
        "    y_pred=models[i].predict(x_test)\n",
        "    if i!='SGD' and i!='KMeans':\n",
        "        results[models[i]]=accuracy_score(y_test, y_pred)\n",
        "    print(classification_report(y_test,y_pred,digits=5))\n",
        "    model_names.append(i)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "    precisions_w.append(precision_score(y_test, y_pred,average='weighted'))\n",
        "    recalls_w.append(recall_score(y_test, y_pred,average='weighted'))\n",
        "    f1_scores_w.append(f1_score(y_test, y_pred,average='weighted'))\n",
        "    precisions_m.append(precision_score(y_test, y_pred,average='macro'))\n",
        "    recalls_m.append(recall_score(y_test, y_pred,average='macro'))\n",
        "    f1_scores_m.append(f1_score(y_test, y_pred,average='macro'))\n",
        "top_3_keys_tf = sorted(results, key=lambda k: results[k], reverse=True)[:3]\n",
        "print(\"Top 3 keys with the highest values:\")\n",
        "print(top_3_keys_tf)\n",
        "c1=top_3_keys_tf[0]\n",
        "c2=top_3_keys_tf[1]\n",
        "c3=top_3_keys_tf[2]\n",
        "i='ensemble voting'\n",
        "models['ensemble voting'] = VotingClassifier (estimators=[('clf1',c1), ('clf2',c2),('clf3',c3)], voting='soft')\n",
        "models['ensemble voting'].fit(x_train, y_train)\n",
        "y_pred = models['ensemble voting'].predict(x_test)\n",
        "y_pred=models[i].predict(x_test)\n",
        "print(classification_report(y_test,y_pred,digits=5))\n",
        "model_names.append(i)\n",
        "accuracies.append(accuracy_score(y_test, y_pred))\n",
        "precisions_w.append(precision_score(y_test, y_pred,average='weighted'))\n",
        "recalls_w.append(recall_score(y_test, y_pred,average='weighted'))\n",
        "f1_scores_w.append(f1_score(y_test, y_pred,average='weighted'))\n",
        "precisions_m.append(precision_score(y_test, y_pred,average='macro'))\n",
        "recalls_m.append(recall_score(y_test, y_pred,average='macro'))\n",
        "f1_scores_m.append(f1_score(y_test, y_pred,average='macro'))\n",
        "metrics_df = pd.DataFrame({\n",
        "'Model Name': model_names,\n",
        "'Accuracy': accuracies,\n",
        "'Precision Weighted': precisions_w,\n",
        "'Recall Weighted': recalls_w,\n",
        "'F1-Score Weighted': f1_scores_w,\n",
        "'Precision Macro': precisions_m,\n",
        "'Recall Macro': recalls_m,\n",
        "'F1-Score Macro': f1_scores_m\n",
        "})\n",
        "metrics_df.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
        "metrics_df.to_csv(\"glove.csv\", index=False)"
      ],
      "metadata": {
        "id": "BE1N3H_xpSVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}